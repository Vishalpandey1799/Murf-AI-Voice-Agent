# ğŸ™ï¸ Murf AI Voice Agent Challenge â€“ Day 1 to 7 Progress

Welcome to my journey through the **Murf AI Voice Agent 30-Day Challenge**!
I'm building a smart and interactive **voice agent** using Murf AI's powerful TTS capabilities and integrating it with real-time tech like **AssemblyAI, FastAPI**, and **LLM APIs**.

---

### ğŸ—“ï¸ Day 1 â€“ Kickoff & Setup

- ğŸš€ Joined the **Murf AI Voice Agent Challenge**
- ğŸ§  Explored the challenge format, goals, and tools
- ğŸ’» Set up the base project using **FastAPI**
- ğŸ” Registered and tested the **Murf API key**
- ğŸ‰ Successfully generated my **first TTS audio** with Murf AI

---

### ğŸ—“ï¸ Day 2 â€“ TTS API Integration

- ğŸ” Connected **Murf's TTS API** to FastAPI backend
- ğŸ§ª Built a basic UI with text input and audio playback
- ğŸ§ Achieved full **text-to-speech cycle** in browser
- ğŸ› ï¸ Handled errors gracefully on both front and back end
- ğŸ“¢ Shared my progress on LinkedIn with `#30DayVoiceAgent`

---

### ğŸ—“ï¸ Day 3 â€“ Voice Agent UX

- ğŸ–Œï¸ Polished the UI with improved design (HTML/CSS)
- ğŸ”„ Refactored API flow for smoother UX
- ğŸ’¡ Learned how to make the voice interaction feel more natural
- ğŸ™Œ Thanked **Murf AI** publicly for enabling student creativity

---

### ğŸ—“ï¸ Day 4 â€“ Echo Bot ğŸ¤

- ğŸª Added a brand-new feature: **Echo Bot** section in the UI
- ğŸ§© Used the browserâ€™s **MediaRecorder API** to:

  - Start and stop mic recordings
  - Instantly play back recorded audio

- ğŸ§  Learned how to work with real-time audio in the browser
- âœ¨ This will serve as the foundation for future **speech input** integration

---

### ğŸ—“ï¸ Day 5 â€“ Audio Upload + Server Integration â˜ï¸

- âºï¸ Extended the Echo Bot to **upload audio to my Python server**
- ğŸ› ï¸ Built a new `/upload` API in **FastAPI** to:

  - Accept audio blob from frontend
  - Save it in an `/uploads` folder
  - Return file **name**, **type**, and **size**

- ğŸ”” Added a real-time **status message on UI** after upload
- ğŸ”ƒ Improved end-to-end interactivity from mic â†’ server â†’ playback
- ğŸ”¥ Feels like Iâ€™m building the foundation for a real voice assistant!

---

### ğŸ—“ï¸ Day 6 â€“ Transcription Integration âœï¸

- ğŸ§µ Created a new endpoint `/transcribe/file` on the FastAPI backend
- ğŸ“¤ This endpoint accepts an uploaded audio file and returns its **transcription**
- ğŸ–¥ï¸ Integrated the new transcription API into the frontend
- ğŸ“œ Displayed the **transcribed text** dynamically in the UI after recording upload
- ğŸ§  Now I have full flow: **record voice â†’ upload audio â†’ transcribe â†’ display text**
- ğŸš€ This brings the project one step closer to **real voice-based interaction**

---

### ğŸ—“ï¸ Day 7 â€“ Voice-to-Voice with /tts/echo Endpoint ğŸ¤ğŸ”„ğŸ™ï¸

- ğŸ†• Created a new backend endpoint **`/tts/echo`** in FastAPI
- ğŸ™ï¸ This endpoint:
  1. Accepts an **audio file** from the client
  2. Uses **AssemblyAI** to transcribe the speech to text
  3. Sends the transcription to **Murf AI** to generate **a new voice**
  4. Saves the generated voice file and returns its **URL** to the client
- ğŸ§ On the client side:
  - After recording stops, the recorded audio is sent to `/tts/echo`
  - The returned **Murf voice URL** is set as the `<audio>` source
  - The new voice plays automatically
- ğŸ”„ Now we have a **full voice-to-voice pipeline**:

  **User speaks â†’ Server transcribes â†’ Murf re-voices â†’ Client plays**

- ğŸš€ This makes the Echo Bot truly interactive and feels like talking to a real AI agent

---

# ğŸš€ Day 8 â€” Building LLM Query Endpoint with FastAPI

## ğŸ“… Overview
On **Day 8** of my Generative AI learning journey, I focused on creating a new FastAPI endpoint that accepts text input, queries an LLM (Google Gemini API), and returns an AI-generated response in JSON format.  
This marks a key step towards integrating AI into APIs for use in chatbots, voice assistants, and other applications.

---

## ğŸ›  Features Implemented
- **POST `/llm/query` endpoint**:
  - Accepts a JSON payload containing a `text` field.
  - Passes the input to an LLM (Google Gemini API).
  - Returns the AI's response in JSON format.

- **LLM Integration**:
  - Used Googleâ€™s **`gemini-1.5-flash`** model for fast response generation.
  - Implemented error handling for model name mismatches and API issues.

- **Code Modularization**:
  - Created a `getResponseFromGemini()` helper function for cleaner code.
  - Isolated API logic from route handling.

---

 


## âš™ï¸ What Youâ€™ll Need to Build This (So Far)

To build your own voice agent or Echo Bot like this, youâ€™ll need:

- âœ… **FastAPI** â€“ For serving APIs
- âœ… **Murf AI account** â€“ To get your TTS API key
- âœ… **AssemblyAI / other transcription API** â€“ For speech-to-text
- âœ… **Browser with MediaRecorder API support** â€“ Chrome, Firefox, etc.
- âœ… **Basic frontend setup** â€“ HTML, CSS, JS
- âœ… **Python + pip** â€“ For installing FastAPI and other packages
- âœ… **.env file** â€“ To store your Murf API Key securely
- âœ… **VS Code + Live Server extension** _(optional)_ â€“ For quick frontend preview

---

## ğŸ’¡ Tools I'm Using

| Tool             | Purpose                             |
| ---------------- | ----------------------------------- |
| Murf AI          | Text-to-Speech (TTS)                |
| FastAPI          | Backend API server                  |
| HTML/CSS/JS      | UI for interaction and playback     |
| MediaRecorder    | Echo Bot mic capture + playback     |
| FormData         | Uploading audio blob to the backend |
| AssemblyAI / STT | Transcribing recorded audio         |

---

## ğŸ™Œ Special Thanks

Huge thanks to **Murf AI** for organizing this amazing challenge and encouraging builders to explore the world of voice-first interfaces.
Your tools are enabling the next generation of interactive agents ğŸ’œ

---

## ğŸ”— Follow My Progress

ğŸ“ Catch my updates on LinkedIn with: [#30DayVoiceAgent](https://www.linkedin.com/in/vishal-kumar-3835a9330/)
Letâ€™s build cool voice stuff together!

---
